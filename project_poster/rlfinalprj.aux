\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Non-Adversarial Case}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The sequential model for estimating wirelessly transmitted binary data across a noisy wireless channel\relax }}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training reaches a maximum reward very quickly as exploration ceases in this simple problem. Testing average over 100 episodes gave $31.30$ reward ($\sim 2.5 \times 10^{-14}$ BER)\relax }}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Adversarial Case}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An example state with actions chosen $a_t : \{ (gain=1,f_c=0.5) \}$. Tone exists at $.83 Hz$, hopping tones at $1.16, 1.5 Hz$, and wide band signal from $1.2 Hz$ to $2 Hz$. SINR exceeds $0 dB$, so detection and jamming occurs.\relax }}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Training reaches a maximum reward more slowly. Testing average over 100 episodes gave $9$ reward ($\sim 1 \times 10^{-4}$ BER). A larger sliding window for the average is used to capture the smaller margin between good and bad performance ($\sim 1$ and $\sim 5-10$ average reward per time step instead of $\sim 1$ and $\sim 36$ non-adversarial margin). DQN additionally learned to avoid wide band interference, SINR equal.\relax }}{1}}
